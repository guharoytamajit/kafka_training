#!/bin/bash

# Make sure you change the ADV_HOST variable in docker-compose.yml
# if you are using docker Toolbox

# 1) Source connectors
# Start our kafka cluster
docker-compose up kafka-cluster
# Wait 2 minutes for the kafka cluster to be started

###############
# A) FileStreamSourceConnector in standalone mode
# Look at the source/demo-1/worker.properties file and edit bootstrap
# Look at the source/demo-1/file-stream-demo.properties file
# Look at the demo-file.txt file


# Make sure you are in >>>C:\Kafka\Kafka-Connect-Lab\code
# open another docker shell
# go to C:\Kafka\Kafka-Connect-Lab\code
# use "docker ps -a" to get ladoop image name
it will looks like this
$ docker ps -a
CONTAINER ID        IMAGE                           COMMAND                  CREATED             STATUS              PORTS																			NAMES
5f9ba31cdf83        landoop/fast-data-dev:cp3.3.0   "/usr/local/bin/dumb"   About an hour ago   Up 41 minutes       0.0.0.0:2181->2181/tcp, 0.0.0.0:3030->3030/tcp, 0.0.0.0:8081-8083->8081-8083/tcp, 0.0.0.0:9092->9092/tcp, 0.0.0.0:9581-9585->9581-9585/tcp, 3031/tcp   code_kafka-cluster_1

Here we will "landoop/fast-data-dev:cp3.3.0" as image map
# We start a hosted tools, mapped on our code
# Linux / Mac
#docker run --rm -it -v "$(pwd)":/tutorial --net=host landoop/fast-data-dev bash
# Windows Command Line:
#docker run --rm -it -v %cd%:/tutorial --net=host landoop/fast-data-dev:cp3.3.0 bash
# Windows Powershell:
#docker run --rm -it -v ${PWD}:/tutorial --net=host landoop/fast-data-dev:cp3.3.0 bash
#Use below
Goutam@pcadmin-PC MINGW64 /c/kafka/Kafka-Connect-Lab/code
$ docker cp . 891c5e5d7e69:/k-connect


# we launch the kafka connector in standalone mode:
cd /tutorial/source/demo-1
# create the topic we write to with 3 partitions
#kafka-topics --create --topic demo-1-standalone --partitions 3 --replication-factor 1 --zookeeper 127.0.0.1:2181
kafka-topics --create --topic demo-1-standalone --partitions 3 --replication-factor 1 --zookeeper 192.168.99.100:2181

----------------
 docker exec -it 891 bash
-----------------
---------------------------------------------------------------------------------------
root@fast-data-dev demo-1 $ kafka-topics --create --topic demo-1-standalone --pa
rtitions 3 --replication-factor 1 --zookeeper 192.168.99.100:2181
Created topic "demo-1-standalone".
---------------------------------------------------------------------------------------


# Usage is connect-standalone worker.properties connector1.properties [connector2.properties connector3.properties]
connect-standalone worker.properties file-stream-demo-standalone.properties

"Address not available" ---- error
change worker.properties
rest.host.name=192.168.99.100  to rest.host.name=<container_id>
# write some data to the demo-file.txt !
# shut down the terminal when you're done.
###############

###############
# B) FileStreamSourceConnector in distributed mode:
# create the topic we're going to write to
docker run --rm -it --net=host landoop/fast-data-dev:cp3.3.0 bash
kafka-topics --create --topic demo-2-distributed --partitions 3 --replication-factor 1 --zookeeper 127.0.0.1:2181
# you can now close the new shell

# head over to 127.0.0.1:3030 -> Connect UI
# Create a new connector -> File Source
# Paste the configuration at source/demo-2/file-stream-demo-distributed.properties

# Now that the configuration is launched, we need to create the file demo-file.txt
docker ps
docker exec -it <containerId> bash
touch demo-file.txt
echo "hi" >> demo-file.txt
echo "hello" >> demo-file.txt
echo "from the other side" >> demo-file.txt

# Read the topic data
docker run --rm -it --net=host landoop/fast-data-dev bash
kafka-console-consumer --topic demo-2-distributed --from-beginning --bootstrap-server 127.0.0.1:9092
# observe we now have json as an output, even though the input was text!
###############

###############
# C) TwitterSourceConnector in distributed mode:
# create the topic we're going to write to
docker run --rm -it --net=host landoop/fast-data-dev bash
kafka-topics --create --topic demo-3-twitter --partitions 3 --replication-factor 1 --zookeeper 127.0.0.1:2181
# Start a console consumer on that topic
kafka-console-consumer --topic demo-3-twitter --bootstrap-server 127.0.0.1:9092

# Follow the instructions at: https://github.com/Eneco/kafka-connect-twitter#creating-a-twitter-application
# To obtain the required keys, visit https://apps.twitter.com/ and Create a New App. Fill in an application name & description & web site and accept the developer aggreement. Click on Create my access token and populate a file twitter-source.properties with consumer key & secret and the access token & token secret using the example file to begin with.

# Setup instructions for the connector are at: https://github.com/Eneco/kafka-connect-twitter#setup
# fill in the required information at demo-3/source-twitter-distributed.properties
# Launch the connector and start seeing the data flowing in!
